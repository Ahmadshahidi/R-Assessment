{
    "collab_server" : "",
    "contents" : "---\ntitle: \"assessmentPap\"\noutput: pdf_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nsource(\"~/Assessment/FunRs.R\")\n```\n\n##Question 1\nThere are 39644 record in the data set. I checked different ways each giving me the same result, indicating that the records are unique. For example: \n```{r reading}\nNewsData <- read.csv(\"~/Assessment/Data/OnlineNewsPopularity/OnlineNewsPopularity.csv\", stringsAsFactors = FALSE)\ndim(NewsData)\nsum(duplicated(NewsData))\n```\nThere are 39644 urls in the data, and time frame is \"2013-01-07\" \"2014-12-27\".\n```{r urls}\nlibrary(stringr)\nurls <- NewsData$url\nlength(urls)\nx <-as.Date(str_extract(urls[1:length(urls)],\"[0-9]{4}/[0-9]{2}/[0-9]{2}\"),\"%Y/%m/%d\")\nx<-sort(x)\nx[c(1,length(urls))]\n```\n##Question 2\n```{r histogram}\nhist(NewsData$timedelta, right = FALSE, col = \"cyan\", main = \"Histogram of timedelta column\")\n```\n\nThe histogram indicates an almost uniform distribution of timedelta.\n\nA part of the question asks *does it changes overtime*. It is not very obvious what \"it\" refers to. Does the histogram, distribution of the acquisition time (timedeta) change? Does the acquisition time itself changes?\nHere I have the monthly average of timedeta:\n```{r monthlyAvetime}\nlibrary(tidyverse)\nlibrary(knitr)\nDate <-as.Date(str_extract(urls[1:length(urls)],\"[0-9]{4}/[0-9]{2}/[0-9]{2}\"),\"%Y/%m/%d\")\nNewsDataD <- cbind.data.frame(NewsData,Date,year = as.numeric(format(Date, format = \"%Y\")),\n                 month = as.numeric(format(Date, format = \"%m\")),\n                 day = as.numeric(format(Date, format = \"%d\")))\nby_month <- group_by(NewsDataD,year,month)\n  res <-summarise(by_month, Avetime = mean(timedelta))\n  kable(res)\n```\nand we can plot this data:\n```{r ploting}\nlibrary(zoo)\nres$Date <- as.yearmon(paste(res$year,res$month, sep = \"-\"))\nhead(res,2)\nplot(res$Avetime~res$Date)\n```\n\nThere is a steep drop in the days until acqusition as time has gone by. This makes sense since the publisher has gotten more mature and gained more experience and the delay between publishing time and acqusition time has decreased. \n\n\n\n## Question 3\nThe function *topic_extract* is written to extract the topic, as it is defined in question 3, from a given url:\n```{r extractTopics}\nExtractedTopics <- lapply(urls,topic_extract) %>% unlist()\nhead(ExtractedTopics,4)\nsum(duplicated(ExtractedTopics))\n```\nI built a frequency table, used loops to detect and count any possible multiple occurances of a topic, and checked with duplicated function of R. All indicate that there is no multi-occurance and the frequency of each topic is exactly 1. \n\n##Question 4\nThe little one liner function *is_it_there* returns TRUE if a substring is in a given string, if not it returns FALSE. We use this function to answer this question.\n```{r substring_in_there}\ns <- c(\"elon-musk\", \"facebook\", \"ebola\", \"ipad\", \"iphone\", \"tornado\", \"sharknado\", \"taylor-swift\")\nres <- lapply(ExtractedTopics,is_in_there,s)\nx <- setNames(do.call(rbind.data.frame,res),s)\nt <- apply(x,2,sum)\nkable(t)\n```\n\nI think the results makes sense. For example, Facebook is a very popular webpage and one should think there would be a lot of news worthy events which make Facebook appear in the news very often. Adding to this, is the period which the data covers. This is the time period in which facebook filed for IPO and went public. The debut was a little bumpy and stock prices gyrated which all contributed to facebook being in the news quite a bit. \n\nAnother example is *ebola*. This was the period in which there was a severe outbreak of the desease that made the word very news worthy. \n\n##Question 5\nFirst the groupping and calculations:\n\n```{r monthlyGrain}\nlibrary(lubridate)\nNewsDataDT <- cbind.data.frame(NewsDataD,ExtractedTopics)\nx <- with(NewsDataDT,split(ExtractedTopics,list(year,month)))\nsn <- paste(names(x),\"1\",sep = \".\") %>% ymd() %>% sort()\nsn <- sub(\"\\\\.0\",\".\",sub(\"-\",\".\",sub(\"-01$\",\"\",sn)))\n```\n\nWe can build monthly tables. The following snippet produces the table that can be later be extracted and presented if neccessary.\n```{r monthlytables}\ntables <- list()\ns <- c(\"elon-musk\", \"facebook\", \"ebola\", \"ipad\", \"iphone\", \"tornado\", \"sharknado\", \"taylor-swift\")\nfor(n in sn){\n  res <- lapply(as.vector(x[[n]]),is_in_there,s)\n  df <- setNames(do.call(rbind.data.frame,res),s)\n  t <- apply(df,2,sum)\n  t <- as.data.frame(t)\n  tables[[n]] <- t\n}\n```\nHere are the a few of the tables:\n\n```{r tables}\nlibrary(gridExtra)\nfor(i in 1:4)\ngrid.arrange(tableGrob(tables[[i]],cols = sn[i]),nrow = 1)\n```\n\nThe frequency changes from month-to-month based on events that happened in that month. For example a month in which a new iphone has been released shows a spike in the iphone frequency. Here is two graphs showing the changes in frequency over time. Again, we can see from this graph that there are jumps in popularity around the times of major events. The noticable exception is facebook that enjoys a high ans stable popularity over time. \n\n```{r graphs}\nx <- with(NewsDataDT,split(ExtractedTopics,list(year,month)))\nsd <- paste(names(x),\"1\",sep = \".\") %>% ymd() %>% sort()\nsd <- as.Date(sd)\n\np1 <- c(\"ipad\",\"ebola\",\"iphone\",\"facebook\")\nNumof_ph <- lapply(p1,phrase_num)\ndf <- data.frame(Numof_ph[[1]]$Date)\nfor(i in 1:length(Numof_ph)){\n  df <- cbind.data.frame(df,Numof_ph[[i]][,2])\n}\nn <- c(\"Date\",p1)\ncolnames(df) <- n\ndf$Date <- sd\nxx <- df %>% gather(name,count,ipad:facebook)\ng1 <- ggplot(data = xx, aes(x = Date, y = count, group = name, colour = name)) +geom_line()+geom_point()+scale_x_date(date_labels = \"%b-%y\")+xlab(\"\")\n\n\np2 <- c(\"elon-musk\",\"tornado\",\"sharknado\",\"taylor-swift\")\nNumof_ph2 <- lapply(p2,phrase_num)\ndf2 <- data.frame(Numof_ph2[[1]]$Date)\nfor(i in 1:length(Numof_ph2)){\n  df2 <- cbind.data.frame(df2,Numof_ph2[[i]][,2])\n}\nn2 <- c(\"Date\",p2)\ncolnames(df2) <- n2\ndf2$Date <- sd\nxx2 <- df2 %>% gather(name,count,2:5)\ng2 <- ggplot(data = xx2, aes(x = Date, y = count, group = name, colour = name)) +geom_line()+geom_point()+scale_x_date(date_labels = \"%b-%y\")+xlab(\"\")\n\n#putting two graphs together\ngrid.arrange(g1, g2, ncol=1)\n```\n\n\n##Question 6\nWe first perform that calculations and then follow that with a discussion. \n```{r Q6}\ndayN <- c(\"weekday_is_sunday\", \"weekday_is_monday\", \"weekday_is_tuesday\",\"weekday_is_wednesday\",\"weekday_is_thursday\",\"weekday_is_friday\",\"weekday_is_saturday\")\ndayData <- NewsData[,dayN]\ndf <- apply(dayData,2,sum)\n#sum(df/7)\nkable(df, caption = \"Total urls\")\n```\n\nFrom the data one can observe that number of shared urls are almost the same during the weekdays, but start to go down on friday. During the weekend activities are less than half of a typical weekday. This finding is what one normally expects. \n\nAnother way to see this result is to look at the averages. On average 5663 URLs are shared each day. The weekend averge of 2595 is well below this number and the weekday average of 6891, is well above it.\n\nFor others we use a little function in FunRs.R, *Forq6_8* to summerize the results and answer the questions.\n\n```{r aveNumViod}\nresults <- Forq6_8(\"num_videos\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"num_videos\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\nFor Average num_images:\n\n```{r avenumim}\nresults <- Forq6_8(\"num_imgs\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"num_imgs\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\nFor Average abs_title_subjectivity:\n\n```{r AveSubj}\nresults <- Forq6_8(\"abs_title_subjectivity\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"abs_title_subjectivity\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\nAverage abs_title_sentiment_polarity:\n\n```{r AvePol}\nresults <- Forq6_8(\"abs_title_sentiment_polarity\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"abs_title_sentiment_polarity\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\nLooking at the results we realize that overall activity on the weekend is well below the activity on the weekdays for all these categories. \n Here is summaraized results:\n \n```{r summary}\nType_of_Ave <-c(\"Daily_Ave\",\"Weekday_Ave\",\"Weekend_Ave\")\nTotal_urls <- c(5663,6891,2595)\nnum_images <- c(25735.43,30129.6,14750)\nnum_videos <- c(7078.571,8789,2802.5)\nabs_title_subjectivity <- c(1936,2373.25,842.88)\nabs_title_sentiment<- c(883.86,1053.32,460.19)\nAverages <- rbind.data.frame(Total_urls,num_images,num_videos,abs_title_subjectivity,abs_title_sentiment)\ncolnames(Averages)<-Type_of_Ave\nAverages <- cbind.data.frame(Parameter = c('Total_urls','num_images','num_videos','abs_title_subjectivity','abs_title_sentiment'),Averages)\nkable(Averages)\nmeltedData <- Averages %>% gather(Type,Values,2:4)\ng3 <- ggplot(meltedData\n             ,aes(Parameter, Values, fill = Type))+geom_bar(stat ='identity', position = 'dodge')\ng3 <- g3+scale_x_discrete(labels = c(\"title_senti\",\"title_subj\",\"num_imgs\",\"num_vid\",\"total_urls\"))+xlab(\"\")\ng3\n```\n\n##Question 7\nFirst we perform the calculations and then will analyze the results. These are for data channels of Entertainment, Lifestyle, Tech and World. Since they all use the same code, I suppress the codes to be printed.\n\n```{r, ch_is_lifestyle, echo=FALSE}\nresults <- Forq6_8(\"data_channel_is_lifestyle\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"data_channel_is_lifestyle\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\n```{r,echo =FALSE}\nresults <- Forq6_8(\"data_channel_is_entertainment\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"data_channel_is_entertainment\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\n\n```{r, echo=FALSE}\nresults <- Forq6_8(\"data_channel_is_tech\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"data_channel_is_tech\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\n```{r,echo=FALSE}\nresults <- Forq6_8(\"data_channel_is_world\")\nDaily_Ave <- results$aved\nWeekday_Ave <- results$avweekd\nWeekend_Ave <- results$avweekendd\nkable(results$dayofweek,caption = \"data_channel_is_world\")\nDaily_Ave\nWeekday_Ave\nWeekend_Ave\n```\n\nAgain the observation is the same. The activities in the weeked days are substantially below those of weekdays. \n\nHere is summary of the results:\n```{r summ}\ndata_channel_is_lifestyle <-c(299.86,341.4,196)\ndata_channel_is_entertainment <-c(1008.14,1228.2,458)\ndata_channel_is_tech<-c(1049.43,1285,460.5)\ndata_channel_is_world<-c(1203.86,1468.2,543)\nAverages <- rbind.data.frame(data_channel_is_lifestyle,data_channel_is_entertainment,data_channel_is_tech,data_channel_is_world)\ncolnames(Averages)<-Type_of_Ave\nAverages <- cbind.data.frame(Parameter = c('data_channel_is_lifestyle','data_channel_is_entertainment','data_channel_is_tech','data_channel_is_world'),Averages)\nkable(Averages)\nmeltedData <- Averages %>% gather(Type,Values,2:4)\ng4 <- ggplot(meltedData\n             ,aes(Parameter, Values, fill = Type))+geom_bar(stat ='identity', position = 'dodge')\ng4 <- g4+scale_x_discrete(labels = c(\"lifestyle\",\"entertainment\",\"tech\",\"world\"))+xlab(\"\")\ng4\n```\n\n\n\n##Question 8\nI was expecting to see a difference between results of some of the question 7 and 6. One thinks entertaiment and lifestyle are type of news that people pay more attention in the weekend, and therefore should be a higher sharing of them.\n \nI think after all most people, including media and news works, are off on weekend are they rather to spend time with family and things like that. Therefore the overal activity derops in the weekends. One can guess that some people shift and prepare what should be consumed for the weekends in the weekdays. \n\n\n\n",
    "created" : 1489626844478.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1924720643",
    "id" : "A1192CFD",
    "lastKnownWriteTime" : 1490046637,
    "last_content_update" : 1490046637534,
    "path" : "~/Assessment/paper.Rmd",
    "project_path" : "paper.Rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}